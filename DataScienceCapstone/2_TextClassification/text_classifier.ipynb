{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification for Sentiment Analysis is one of the most common tasks in natural language processing. It consists in computationally analysing text messages and tell whether the underlying sentiment is positive, negative our neutral. Let's create a simple text classifier using tweets from the first 2016 GOP Presidential Debate available online [kaggle](https://www.kaggle.com/ngyptr/python-nltk-sentiment-analysis/data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas\n",
    "tweets_data_df = pd.read_csv(\"https://raw.githubusercontent.com/emmanueliarussi/DataScienceCapstone/master/2_TextClassification/data/Sentiment.csv\") # load data\n",
    "tweets_data_df = tweets_data_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Let's take a look at the content of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 5)  # only display up to 5 rows when printing dataframes (reduce visual clutter)\n",
    "tweets_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the sentiment labels distributed? How much can we trust on those labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns                  # import seaborn for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# bar chart to show the number of tweets tagged as Positive/Neutral/Negative\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.countplot(x=\"sentiment\", order=[\"Negative\",\"Neutral\",\"Positive\"], data=tweets_data_df, palette=\"PiYG\", ax=ax1)\n",
    "ax1.set_title(\"Sentiment Label Counts\")\n",
    "ax1.set_xlabel(\"Sentiment\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "\n",
    "# scatter plot showing the labels confidence per tweet.\n",
    "# usefull for discarding unreliable tweets. \n",
    "sns.scatterplot(x=\"id\", y=\"sentiment_confidence\", hue=\"sentiment_confidence\", palette=\"viridis\",\n",
    "                sizes=(5,), linewidth=0, alpha=0.7,legend=False, data=tweets_data_df, ax=ax2)\n",
    "ax2.set_title(\"Tweet Label Confidence\")\n",
    "ax2.set_xlabel(\"Tweet ID\")\n",
    "ax2.set_ylabel(\"Confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering\n",
    "Based on the figures above, we decide to filter the data we will use in order to build our classifier and keep only `text` and `sentiment` columns. Adittionally, we remove rows with `sentiment_confidence` below 0.5. Since we will be performing binary classification, we remove `Neutral` tweets and replace `Negative` and `Positive` strings with integers labels. Finally, a large unbalance in the number of labeled data for a given category can severely affect the performance of a classiier. Therefore, we randomly remove negative tweets to match the number of positive samples in or data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# keeping only rows with sentiment_confidence >0.5\n",
    "tweets_data_df = tweets_data_df[tweets_data_df[\"sentiment_confidence\"]>0.5] \n",
    "\n",
    "# remove random negative tweets\n",
    "total_negative = tweets_data_df[\"sentiment\"].value_counts()[\"Negative\"] # count positive and negative rows\n",
    "total_positive = tweets_data_df[\"sentiment\"].value_counts()[\"Positive\"]\n",
    "to_remove = np.random.choice(tweets_data_df[tweets_data_df['sentiment']==\"Negative\"].index, # select random rows\n",
    "                             size=total_negative-total_positive,\n",
    "                             replace=False)\n",
    "tweets_data_df = tweets_data_df.drop(to_remove) # remove\n",
    "\n",
    "# keeping only the neccessary columns [['text','sentiment']]\n",
    "tweets_data_df = tweets_data_df[[\"text\",\"sentiment\"]]         \n",
    "# replace sentiment string labels with integers and remove neutral tweets. \n",
    "tweets_data_df = tweets_data_df[tweets_data_df[\"sentiment\"]!=\"Neutral\"]      \n",
    "tweets_data_df[\"sentiment\"][tweets_data_df[\"sentiment\"]==\"Negative\"]  = 0   \n",
    "tweets_data_df[\"sentiment\"][tweets_data_df[\"sentiment\"]==\"Positive\"]  = 1      \n",
    "tweets_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output distribution using a barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart to show the number of tweets tagged as Positive/Neutral/Negative\n",
    "f, (ax1) = plt.subplots(1, 1, figsize=(5, 5))\n",
    "sns.countplot(x=\"sentiment\", order=[0,1], data=tweets_data_df, palette=\"PiYG\", ax=ax1)\n",
    "ax1.set_title(\"Sentiment Label Counts (after filtering)\")\n",
    "ax1.set_xlabel(\"Sentiment\")\n",
    "ax1.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now prepare tweet text to be interpretable by our classifier. Using tools from [NLTK](https://www.nltk.org/) we first remove *stop words* which are words that do not have any important significance to be used in our classifier. Usually these words are also filtered out from search queries because they return vast amount of unnecessary information (such i.e. the, is, at, which, on, for, this, etc.). We also remove usernames, hashtags and URLs. NLTK also provides us with a tweet *tokenizer* module that splits tweet strings into a list of words. Additionally, we employ a *stemmer* to remove morphological affixes from words, leaving only the word stem (i.e.: \"running\" -> \"run\", \"generously\" -> \"generous\"). Finally, we also remove common emojis, hashtags, punctuation and numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus   import stopwords \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem     import PorterStemmer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Set of emojis to remove from tweets\n",
    "emojis = set([ ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "               ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "               '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "               'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)','<3',\n",
    "               ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "               ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "               ':c', ':{', '>:\\\\', ';(', 'ðŸ‡ºðŸ‡¸'])\n",
    " \n",
    "# English stop words set\n",
    "nltk.download('stopwords')\n",
    "stopwords_english  = set(stopwords.words(\"english\"))\n",
    "    \n",
    "# Tokenizer \n",
    "tokenizer    = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "    # regular expression to remove \"RT\" from tweets\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # regular expression to remove \"URLs\" from tweets\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # regular expression to remove \"#\" from tweets\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # regular expression to remove \"@\" from tweets\n",
    "    tweet = re.sub(r'@', '', tweet)\n",
    " \n",
    "    # tokenize\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    " \n",
    "    tweet_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and     # no stopwords\n",
    "            word not in emojis and                # no emojis\n",
    "            word not in string.punctuation):      # no punctuation\n",
    "                stem_word = stemmer.stem(word)    # stemming word\n",
    "                tweet_clean.append(stem_word)\n",
    " \n",
    "    return tweet_clean\n",
    "\n",
    "# get filtered clean words for each tweet\n",
    "tweets_data_df[\"text\"] = tweets_data_df[\"text\"].apply(lambda x: clean_tweet(x))\n",
    "# remove stopwords and punctuation\n",
    "tweets_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a sentiment classifier\n",
    "First, we identify and create a dicctionary of unique words using the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) method from scikit-learn. In order to train a classifier, we need to create a vector identifying each tweet, this is called a feature vector. The vectorizer helps us to translate each word into a unique integer code, extracting the *vocabulary* in our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "phrases    = [' '.join(x) for x in tweets_data_df[\"text\"].values ] # All the phrases in our tweets\n",
    "vectorizer = CountVectorizer() \n",
    "vectorizer = vectorizer.fit(phrases)\n",
    "print(\"Vocabulary size:{}\".format(len(vectorizer.vocabulary_)))\n",
    "print(\"Vocabulary content:{}\".format(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method, we now split the dataset into a training and a testing set. We define a test set about 10% of the original dataset size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for splitting data to train and test sets\n",
    "from sklearn.model_selection import train_test_split            \n",
    "\n",
    "# Splitting the dataset into train and test set\n",
    "train, test = train_test_split(tweets_data_df,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to transform tokens in each partition into a feature vectors. We will use our vectorizer to transform each tweet into a [*bag of words*](https://en.wikipedia.org/wiki/Bag-of-words_model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train              = [' '.join(x) for x in train['text'].values ] \n",
    "X_train_bag_of_words = vectorizer.transform(X_train)\n",
    "y_train              = train['sentiment'].values.astype(int)\n",
    "\n",
    "X_test              = [ ' '.join(x) for x in test['text'].values ] \n",
    "X_test_bag_of_words = vectorizer.transform(X_test)\n",
    "y_test              = test['sentiment'].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing instances from the training dataset after the *bag of words* transformation, we see pairs like `(0, 628)\t1`. The first number identifies the phrase index (the tweet), the second number is the word code asigned by our vectorizer, finally, the las number is the count of occurrences of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing bag-of-words representation\n",
    "print(X_train_bag_of_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "gnb = RandomForestClassifier(class_weight=\"balanced\")\n",
    "gnb.fit(X_train_bag_of_words.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score, classification_report\n",
    "\n",
    "y_pred_test  = gnb.predict(X_test_bag_of_words.toarray())\n",
    "\n",
    "print(\" <================= Testing dataset metrics =================> \")\n",
    "print(\"Number of mislabeled test points out of a total {} points : {}\".format(X_test_bag_of_words.toarray().shape[0],(y_test != y_pred_test).sum()))\n",
    "print(\"Confusion matrix:\\n\",confusion_matrix(y_test, y_pred_test),\"\\n\")\n",
    "print(\"Balanced accuracy score:\",balanced_accuracy_score(y_test, y_pred_test))\n",
    "print(\"Full Report:\\n\",classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
